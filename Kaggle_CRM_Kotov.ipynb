{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preductor\\AppData\\Local\\conda\\conda\\envs\\py3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(r'C:\\Users\\preductor\\Documents\\MachineLearning\\6_course\\Сhurn_prediction\\week_4\\orange_small_churn_train_data.csv',\n",
    "                  engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>...</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3052.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>vr93T2a</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>xwM2aC7IdeMC0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1813.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6hQ9lNX</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ELof</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>55YFVY9</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>catzS2D</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>ZI9m</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>e4lqvY0</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>686.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>MAz3HNj</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WqMG</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9   ...    \\\n",
       "0   0   NaN   NaN   NaN   NaN   NaN  3052.0   NaN   NaN   NaN   ...     \n",
       "1   1   NaN   NaN   NaN   NaN   NaN  1813.0   7.0   NaN   NaN   ...     \n",
       "2   2   NaN   NaN   NaN   NaN   NaN  1953.0   7.0   NaN   NaN   ...     \n",
       "3   3   NaN   NaN   NaN   NaN   NaN  1533.0   7.0   NaN   NaN   ...     \n",
       "4   4   NaN   NaN   NaN   NaN   NaN   686.0   7.0   NaN   NaN   ...     \n",
       "\n",
       "    Var222      Var223  Var224  Var225  Var226   Var227         Var228  \\\n",
       "0  vr93T2a  LM8l689qOp     NaN     NaN    fKCe  02N6s8f  xwM2aC7IdeMC0   \n",
       "1  6hQ9lNX  LM8l689qOp     NaN    ELof    xb3V     RAYp        55YFVY9   \n",
       "2  catzS2D  LM8l689qOp     NaN     NaN    FSa2     ZI9m  ib5G6X1eUxUn6   \n",
       "3  e4lqvY0  LM8l689qOp     NaN     NaN    xb3V     RAYp  F2FyR07IdsN7I   \n",
       "4  MAz3HNj  LM8l689qOp     NaN     NaN    WqMG     RAYp  F2FyR07IdsN7I   \n",
       "\n",
       "   Var229  Var230  labels  \n",
       "0     NaN     NaN      -1  \n",
       "1    mj86     NaN      -1  \n",
       "2    mj86     NaN      -1  \n",
       "3     NaN     NaN       1  \n",
       "4     NaN     NaN      -1  \n",
       "\n",
       "[5 rows x 232 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент churn обьектов: 7.44%\n"
     ]
    }
   ],
   "source": [
    "print('Процент churn обьектов: {}%'.format(round(raw_data[raw_data.labels==1].shape[0]/raw_data.shape[0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определим данные с большим количеством NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Var52</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var141</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var55</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nans\n",
       "Var52   100\n",
       "Var141  100\n",
       "Var55   100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_of_nan = pd.DataFrame(index = raw_data.columns, columns=['nans'])\n",
    "for col in raw_data.columns:\n",
    "    table_of_nan.loc[col,'nans'] = round((raw_data[col].isna().sum())/raw_data[col].shape[0]*100, 2)\n",
    "table_of_nan.sort_values(by='nans', ascending=False, inplace=True)\n",
    "table_of_nan.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_nans(percentage):    \n",
    "    print('Признаков, где Nan объектов меньше {}%: {}%'.format(percentage,\n",
    "                    round(100*table_of_nan[table_of_nan.nans<percentage].shape[0]/table_of_nan.shape[0], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего Nan значений в данных 69.18%\n"
     ]
    }
   ],
   "source": [
    "print('Всего Nan значений в данных {}%'.format(round(table_of_nan.nans.sum()/table_of_nan.shape[0],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Признаков, где Nan объектов меньше 98%: 68.53%\n"
     ]
    }
   ],
   "source": [
    "percent_nans(98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Теперь я попытаюсь собрать в кучу наброски из прошлых недель и составить удобные функции для дальнейшего комбинирования различных вариантов обучения нашей модели, а затем и их тестирования </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первым делом создам функцию отсеивающую признаки, в которых не определен заданный процент значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_attr(percentage):\n",
    "    temp = raw_data.loc[:,table_of_nan[table_of_nan.nans<percentage].index]\n",
    "    X, hold_out_X, y, hold_out_y = train_test_split(temp.drop('labels',axis=1),\n",
    "                                                temp.labels, stratify=temp.labels, test_size=0.2)\n",
    "    data = X\n",
    "    data['y'] = y\n",
    "    hold_out = hold_out_X\n",
    "    hold_out['y'] = hold_out_y\n",
    "    return data, hold_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, hold_out = filter_attr(75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделим вещественные и категориальные данные из отфильтрованных данных для дальнейшей обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_processing(data):\n",
    "    num_cols = []\n",
    "    cat_cols = []\n",
    "    for col in data.drop(columns=['ID','y'], axis=1).columns:\n",
    "        if (int(str(col)[3:])<190):\n",
    "            num_cols.append(col)\n",
    "        else:\n",
    "            cat_cols.append(col)\n",
    "    return data.loc[:,num_cols], data.loc[:,cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num, X_cat = type_processing(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Начну с вещественных данных, к которым применю imputer с заданным типом наполнения Nan значений,  а затем стандартизирую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_processing(X_num, strategy):\n",
    "    imp = Imputer(strategy=strategy, axis=0)\n",
    "    X_num_imputed = imp.fit_transform(X_num)\n",
    "    return pd.DataFrame(data=StandardScaler().fit_transform(X_num_imputed), index=X_num.index,\n",
    "                         columns=X_num.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### К категориальным применю разные подходы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Var211</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var201</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Var208</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        amount\n",
       "Var211       2\n",
       "Var201       3\n",
       "Var208       3"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#таблица количества уникальных значений\n",
    "number_of_unique = pd.DataFrame(index=X_cat.columns, columns=['amount'])\n",
    "for col in X_cat.columns:\n",
    "    number_of_unique.loc[col,'amount'] = X_cat.loc[:,col].unique().shape[0]\n",
    "number_of_unique.sort_values(by='amount', inplace=True)\n",
    "number_of_unique.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В первом случае просто выкину все признаки у которых уникальных значений меньше 10 и применю OneHotEncoding к оставшимся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_1(X_cat):\n",
    "    try:\n",
    "        return pd.get_dummies(X_cat.loc[:, number_of_unique[number_of_unique.amount<10].index])        \n",
    "    except:\n",
    "        print('Не осталось категориальных признаков с кол-вом значений меньше 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Во втором - оставлю еще и те признаки, уникальных значений в которых меньше 50 и применю к ним LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_2(X_cat):\n",
    "    try:\n",
    "        X_cat_less_10 = pd.get_dummies(X_cat.loc[:, number_of_unique[number_of_unique.amount<10].index])\n",
    "        X_cat_labelen = X_cat.loc[:, number_of_unique[50>number_of_unique.amount]\n",
    "                              [number_of_unique.amount>=10].index].astype('str') \\\n",
    "                                    .apply(LabelEncoder().fit_transform)\n",
    "        return pd.concat([X_cat_less_10, X_cat_labelen], axis=1)\n",
    "    except:\n",
    "        print('Не осталось категориальных признаков с количеством значений меньше 50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В третьем - не буду ничего предпринимать, посмотрим как поведет себя LightGBM на сырых категориальных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble(percentage, strategy, num_of_cat_preprocessing=3):\n",
    "    data, hold_out = filter_attr(percentage)\n",
    "    # ------------------------------------------\n",
    "    X_num, X_cat = type_processing(data)\n",
    "    X_num_pro = num_processing(X_num, strategy)\n",
    "    X_cat_pro = X_cat.astype('category')\n",
    "    if (num_of_cat_preprocessing==1):\n",
    "        X_cat_pro = cat_1(X_cat)\n",
    "    elif (num_of_cat_preprocessing==2):\n",
    "        X_cat_pro = cat_2(X_cat)\n",
    "    data_pro = pd.concat([X_num_pro, X_cat_pro], axis=1)\n",
    "    \n",
    "    # hold out тоже сразу предобработаем -------\n",
    "    X_num, X_cat = type_processing(hold_out)\n",
    "    X_num_pro = num_processing(X_num, strategy)\n",
    "    X_cat_pro = X_cat.astype('category')\n",
    "    if (num_of_cat_preprocessing==1):\n",
    "        X_cat_pro = cat_1(X_cat)\n",
    "    elif (num_of_cat_preprocessing==2):\n",
    "        X_cat_pro = cat_2(X_cat)\n",
    "    hold_out_pro = pd.concat([X_num_pro, X_cat_pro], axis=1)\n",
    "    \n",
    "    return data_pro, data.y, hold_out_pro, hold_out.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>На обучении буду рассматривать Logistic Regression, XGBoost и LightGBM</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(estimator, param_grid, percentage, fit_params=None, strategy='mean',\n",
    "         num_of_cat_preprocessing=3):\n",
    "    train_X, train_y, test_X, test_y = assemble(percentage, \n",
    "                                                          strategy, num_of_cat_preprocessing)\n",
    "    gscv = GridSearchCV(estimator,\n",
    "             param_grid=param_grid, fit_params=fit_params,\n",
    "             cv=StratifiedShuffleSplit(n_splits=3, test_size=0.25),\n",
    "             scoring='roc_auc',\n",
    "             verbose=False)\n",
    "    gscv.fit(train_X, train_y)\n",
    "    hold_out_score = np.mean(cross_val_score(gscv.best_estimator_, test_X, test_y,\n",
    "            scoring='roc_auc', cv = StratifiedShuffleSplit(n_splits=3, test_size=0.25)))\n",
    "    print('На данных с менее {}% Nans'.format(percentage))\n",
    "    print('Score on train dataset: {}'.format(gscv.best_score_))\n",
    "    print('Score on hold_out dataset: {}'.format(hold_out_score))\n",
    "    print(gscv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_rate = [75,90,98,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_1 = {'C': [0.1,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На данных с менее 75% Nans\n",
      "Score on train dataset: 0.6738146115975087\n",
      "Score on hold_out dataset: 0.6939655328699524\n",
      "{'C': 0.1}\n",
      "На данных с менее 90% Nans\n",
      "Score on train dataset: 0.6882505385678918\n",
      "Score on hold_out dataset: 0.6421330993465048\n",
      "{'C': 0.1}\n",
      "На данных с менее 98% Nans\n",
      "Score on train dataset: 0.6905468142692593\n",
      "Score on hold_out dataset: 0.6379380152454021\n",
      "{'C': 1}\n",
      "На данных с менее 100% Nans\n",
      "Score on train dataset: 0.6636273847824072\n",
      "Score on hold_out dataset: 0.694760798020781\n",
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "for percentage in per_rate:\n",
    "    test(LogisticRegression(), param_grid_1, percentage, None, 'mean', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_2 = {\n",
    "    'max_depth': [5],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'n_estimators': [500],\n",
    "    'reg_alpha': [0.02]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На данных с менее 75% Nans\n",
      "Score on train dataset: 0.7411626257525292\n",
      "Score on hold_out dataset: 0.7053439884360229\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 90% Nans\n",
      "Score on train dataset: 0.7401636414187551\n",
      "Score on hold_out dataset: 0.7160051341738005\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 98% Nans\n",
      "Score on train dataset: 0.7407386938570162\n",
      "Score on hold_out dataset: 0.7042501967012208\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 100% Nans\n",
      "Score on train dataset: 0.7433507150328058\n",
      "Score on hold_out dataset: 0.7013779358639202\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "for percentage in per_rate:\n",
    "    test(XGBClassifier(), param_grid_2, percentage, None, 'mean', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Посмотрим как классификатор отреагирует на появление LabelEncoder признаков</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На данных с менее 75% Nans\n",
      "Score on train dataset: 0.7394604296816633\n",
      "Score on hold_out dataset: 0.7144351502362228\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 90% Nans\n",
      "Score on train dataset: 0.7402768210592813\n",
      "Score on hold_out dataset: 0.698949839073625\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 98% Nans\n",
      "Score on train dataset: 0.7359260246975224\n",
      "Score on hold_out dataset: 0.7001602616398174\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 100% Nans\n",
      "Score on train dataset: 0.7330198953315291\n",
      "Score on hold_out dataset: 0.7032047493524874\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "for percentage in per_rate:\n",
    "    test(XGBClassifier(), param_grid_2, percentage, None, 'mean', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Мне кажется, результат скорее положительный, еще протестим возможность заполнения Nan значений медианой вместо среднего</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На данных с менее 75% Nans\n",
      "Score on train dataset: 0.7247111025368959\n",
      "Score on hold_out dataset: 0.6799233016315022\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 90% Nans\n",
      "Score on train dataset: 0.7285200589351203\n",
      "Score on hold_out dataset: 0.6863059692022088\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 98% Nans\n",
      "Score on train dataset: 0.7191496395992563\n",
      "Score on hold_out dataset: 0.7284640867685042\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n",
      "На данных с менее 100% Nans\n",
      "Score on train dataset: 0.7284118725140293\n",
      "Score on hold_out dataset: 0.7234120984243356\n",
      "{'colsample_bytree': 0.7, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.01, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "for percentage in per_rate:\n",
    "    test(XGBClassifier(), param_grid_2, percentage, None, 'median', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_3 ={\n",
    "    'num_leaves': [15, 30],\n",
    "    'reg_alpha': [0.01],\n",
    "    'min_data_in_leaf': [30, 50],\n",
    "    'max_depth':[3], \n",
    "    'silent':[True], \n",
    "    'metric':['auc'],                        \n",
    "    'n_estimators':[100],\n",
    "    'colsample_bytree':[0.8],\n",
    "    'subsample':[0.7],\n",
    "    'learning_rate':[0.1]\n",
    "    }\n",
    "fit_params_3 = {\n",
    "#             \"early_stopping_rounds\":10, \n",
    "            \"eval_metric\" : 'auc', \n",
    "#             \"eval_set\" : [(test_X,test_y)],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'feature_name': 'auto'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Сразу попробую использовать LabelEncoder</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "На данных с менее 75% Nans\n",
      "Score on train dataset: 0.7245523786827358\n",
      "Score on hold_out dataset: 0.7388436264574322\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'auc', 'min_data_in_leaf': 50, 'n_estimators': 100, 'num_leaves': 15, 'reg_alpha': 0.01, 'silent': True, 'subsample': 0.7}\n",
      "На данных с менее 90% Nans\n",
      "Score on train dataset: 0.7371766748563032\n",
      "Score on hold_out dataset: 0.7018541280666959\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'auc', 'min_data_in_leaf': 30, 'n_estimators': 100, 'num_leaves': 15, 'reg_alpha': 0.01, 'silent': True, 'subsample': 0.7}\n",
      "На данных с менее 98% Nans\n",
      "Score on train dataset: 0.7479362683628481\n",
      "Score on hold_out dataset: 0.6823489812024941\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'auc', 'min_data_in_leaf': 30, 'n_estimators': 100, 'num_leaves': 15, 'reg_alpha': 0.01, 'silent': True, 'subsample': 0.7}\n",
      "На данных с менее 100% Nans\n",
      "Score on train dataset: 0.7362548584592514\n",
      "Score on hold_out dataset: 0.7042260245081865\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'metric': 'auc', 'min_data_in_leaf': 30, 'n_estimators': 100, 'num_leaves': 15, 'reg_alpha': 0.01, 'silent': True, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "for percentage in per_rate:\n",
    "    test(LGBMClassifier(), param_grid_3, percentage, fit_params_3, 'mean', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Ну и на последок посмотрим как LightGBM справится с категориальными данными</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for percentage in per_rate:\n",
    "    test(LGBMClassifier(), param_grid_3, percentage, fit_params_3, 'mean', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Подытоживая считаю XGBoost очевидным победителем здесь, LightGBM разочаровал на сырых данных, хотя возможно я делаю что-то не так, по идее он должен определять категориальные признаки самостоятельно. Насчет процента Nan значений, с которым работает модель, вижу закономерность в том, что чем большее данных мы отсеиваем тем ближе показатели метрик на train и hold_out выборках.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r'C:\\Users\\preductor\\Documents\\MachineLearning\\6_course\\Сhurn_prediction\\week_4\\orange_small_churn_test_data.csv',\n",
    "                  engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_result(test_X, estimator, param_grid, percentage,\n",
    "                 fit_params=None, strategy='mean', num_of_cat_preprocessing=3):\n",
    "    \n",
    "    \n",
    "    data = raw_data.loc[:,table_of_nan[table_of_nan.nans<percentage].index]\n",
    "    data['y'] = data.labels\n",
    "    data.drop(columns='labels', inplace=True)\n",
    "    train_y = data.y\n",
    " \n",
    "    X_num_1, X_cat_1 = type_processing(data)\n",
    "    X_num_pro_1 = num_processing(X_num_1, strategy)\n",
    "    \n",
    "    data = test_X.loc[:,table_of_nan[table_of_nan.nans<percentage].index]\n",
    "    data['y'] = np.nan\n",
    "    data.drop(columns='labels', inplace=True)\n",
    "\n",
    "    X_num_2, X_cat_2 = type_processing(data)\n",
    "    X_num_pro_2 = num_processing(X_num_2, strategy)\n",
    "\n",
    "    #-------------------------------------\n",
    "    X_cat = pd.concat([X_cat_1, X_cat_2], axis=0)\n",
    "    X_cat_pro = X_cat.astype('category')\n",
    "    if (num_of_cat_preprocessing==1):\n",
    "        X_cat_pro = cat_1(X_cat)\n",
    "    elif (num_of_cat_preprocessing==2):\n",
    "        X_cat_pro = cat_2(X_cat)\n",
    "        \n",
    "    #---------------------------------------------\n",
    "    data_train = pd.concat([X_num_pro_1, X_cat_pro.iloc[:40000,:]], axis=1)    \n",
    "   \n",
    "    #----------------------------------------\n",
    "    data_test = pd.concat([X_num_pro_2, X_cat_pro.iloc[-10000:,:]], axis=1)\n",
    "    # -------------------------------------------------\n",
    "    \n",
    "    gscv = GridSearchCV(estimator,\n",
    "             param_grid=param_grid, fit_params=fit_params,\n",
    "             cv=StratifiedShuffleSplit(n_splits=3, test_size=0.25),\n",
    "             scoring='roc_auc',\n",
    "             verbose=False)\n",
    "    gscv.fit(data_train, train_y)\n",
    "    \n",
    "  \n",
    "    print(gscv.best_score_)\n",
    "    #-------------------------------------------------\n",
    "    return  data_train, train_y, data_test, gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7362583564898715\n"
     ]
    }
   ],
   "source": [
    "data_train, train_y, data_test, clf= final_result(test, LGBMClassifier(), \n",
    "                                param_grid_3, 95, fit_params_3, 'mean', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    9999\n",
       " 1       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(data_train, train_y)\n",
    "prediction = clf.predict(data_test)\n",
    "pd.Series(prediction).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.105210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.026722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    result\n",
       "0   0  0.076600\n",
       "1   1  0.105210\n",
       "2   2  0.026722"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = pd.DataFrame(index=range(len(test.ID)), columns=['ID','result'])\n",
    "ans['result'] = clf.predict_proba(data_test).T[1]\n",
    "ans['ID'] = test.ID\n",
    "ans.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.to_csv(r'C:\\Users\\preductor\\Documents\\MachineLearning\\6_course\\Сhurn_prediction\\week_4\\submission.csv',\n",
    "  sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'>Среди перебранных мной вариантов победил LightGBM на данных с менее 100% Nan значений, OneHotEncoding для кат. признаков с менее 10 уникальными значениями и LabelEncoding для признаков с менее 50 уникальных значений. Вообще отсечение признаков с малым количеством записей по сути не принесло пользы. На Kaggle соревнование закончилось, поэтому делаю принтскрин просто своих результатов без leaderbord</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
